import logging
import os
import pickle
from collections import defaultdict
from email.policy import default
from pathlib import Path
from accelerate.utils import broadcast,wait_for_everyone

import torch
import torch.nn as nn
from transformers import TrainerCallback, TrainingArguments
from typing import Optional, Dict, Any
import os
from trl.extras.profiling import profiling_decorator
import time
import uuid
from pathlib import Path
from typing import Optional
import random
def setup_fs_queue(base_path: str):
    """åˆ›å»ºé˜Ÿåˆ—å’Œå¤„ç†ç›®å½•"""
    queue_dir = Path(base_path) / "queue"
    processing_dir = Path(base_path) / "processing"
    queue_dir.mkdir(parents=True, exist_ok=True)
    processing_dir.mkdir(parents=True, exist_ok=True)
    return queue_dir, processing_dir

@profiling_decorator
def push_to_fs_queue(self, data: Dict[str, Any], time_save):
    """
    å°†åŒ…å« PyTorch å¼ é‡çš„æ•°æ®å­—å…¸åŸå­åœ°å†™å…¥æ–‡ä»¶é˜Ÿåˆ—ã€‚
    ä½¿ç”¨ torch.save è¿›è¡Œåºåˆ—åŒ–ã€‚
    """
    # 1. å†™å…¥ä¸´æ—¶æ–‡ä»¶ã€‚ä½¿ç”¨ .tmp åç¼€ä»¥ç¤ºåŒºåˆ†ã€‚
    # ä½¿ç”¨ torch.save ä¿å­˜æ•°æ®å­—å…¸ã€‚
    # æ³¨æ„ï¼šä¸ºäº†è·¨è¿›ç¨‹å®‰å…¨åœ°åŠ è½½ï¼Œæ‰€æœ‰å¼ é‡åœ¨ä¿å­˜å‰éƒ½åº”è¯¥åœ¨ CPU ä¸Šã€‚
    # (è¿™ä¸ªæ“ä½œåº”è¯¥åœ¨è°ƒç”¨æ­¤å‡½æ•°ä¹‹å‰ï¼Œåœ¨ sampler_script.py ä¸­å®Œæˆ)
    # queue_dir = self.queue_dir / f"{self.model_ids}/{self.rank}"
    # self.queue_dir.mkdir(parents=True, exist_ok=True)
    
    tmp_filename = f"tmp_{uuid.uuid4().hex}.pt"  # ä½¿ç”¨ .pt æ‰©å±•å
    tmp_path = self.queue_dir / tmp_filename

    try:
        torch.save(data, tmp_path)
    except Exception as e:
        print(f"ERROR: Failed to save data to temporary file {tmp_path}. Error: {e}")
        # å¦‚æœä¿å­˜å¤±è´¥ï¼Œæ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if tmp_path.exists():
            tmp_path.unlink()
        raise

    assert self.model_ids == data["model_ids"]
    # 2. åŸå­åœ°é‡å‘½åä¸ºæ­£å¼æ–‡ä»¶ï¼Œè¡¨ç¤ºæ•°æ®å·²å‡†å¤‡å¥½è¢«æ¶ˆè´¹ã€‚
    # è¿™ç§æ–¹å¼å¯ä»¥é˜²æ­¢æ¶ˆè´¹è€…è¯»åˆ°ä¸å®Œæ•´çš„æ–‡ä»¶ã€‚
    final_filename = f"data_{int(time_save*1000)}_SamplerRank_{self.rank}_ModelID_{self.model_ids}_{uuid.uuid4().hex[:6]}.pt"
    final_path = self.queue_dir / final_filename
    wait_for_everyone()
    os.rename(tmp_path, final_path)
    print(f"æ–‡ä»¶ä¿å­˜åœ¨: {final_path}")


def get_rank_from_name(name):
    return int(name.split("_")[3])

def get_model_id_from_name(name):
    return int(name.split("_")[5])

def get_max_model_id(sorted_name_list, rank: int):
    if len(sorted_name_list) == 0:
        return -1, None
    rank_pt_name_list = [file for file in sorted_name_list if get_rank_from_name(file.name) == rank]
    rank_model_ids_list = [get_model_id_from_name(file.name) for file in rank_pt_name_list]
    assert len(rank_model_ids_list) != 0, f"rank_model_ids_list is {rank_model_ids_list}, but sorted_name_list is {sorted_name_list}, rank_pt_name_list is {rank_pt_name_list}"
    max_value = max(rank_model_ids_list)
    max_index = rank_model_ids_list.index(max_value)  # æœ€å¤§å€¼çš„ç´¢å¼•
    return max_value, rank_pt_name_list[max_index]

def get_min_model_id(sorted_name_list, min_id, rank: int):
    if len(sorted_name_list) == 0:
        return -1, None
    rank_pt_name_list = [file for file in sorted_name_list if get_rank_from_name(file.name) == rank]
    for file in rank_pt_name_list:
        valid_id  = get_model_id_from_name(file.name)
        if valid_id >= min_id:
            return valid_id, file

@profiling_decorator
def pop_from_fs_queue(self, queue_dir: Path, processing_dir: Path, rank: int, timeout: int = 600, AIS_len: int = 8, max_diff_step: int = 12, world_size: int=4) -> Optional[Dict[str, Any]]:
    """
    åŸå­åœ°ä»æ–‡ä»¶é˜Ÿåˆ—ä¸­è·å–ä¸€ä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨ torch.load è¯»å–ï¼Œå¹¶è¿”å›å…¶å†…å®¹ã€‚
    è¿™æ˜¯ä¸€ä¸ªé˜»å¡å¼æ“ä½œï¼Œä¸ºå¤šè¿›ç¨‹æ¶ˆè´¹è€…è®¾è®¡ã€‚
    """
    learner_model_id = self.state.global_step
    # print("async_utils.py line 163",self._metrics)

    last_train_model_id = self.last_model_id

    if rank == 0:
        print(f"\nlast_train_model_id:{last_train_model_id}, learner_model_id:{learner_model_id} \n")

    while True:
        sorted_queue_dir = sorted(list(Path(queue_dir).glob("data_*.pt")))
        num_files_of_queue = len(sorted_queue_dir)
        if num_files_of_queue % world_size != 0:
            print(f"æ–‡ä»¶æ•°{num_files_of_queue}ä¸æ˜¯{world_size}å€æ•°ï¼Œè·³è¿‡")
            time.sleep(1.0)
            continue

        sorted_queue_dir = sorted_queue_dir[-256:]
        queue_dir_max_model_id, path_in_queue_max = get_max_model_id(sorted_queue_dir, rank)

        if not path_in_queue_max:
            time.sleep(1.0)  # é˜Ÿåˆ—ä¸ºç©ºï¼ŒçŸ­æš‚ç­‰å¾…åé‡è¯•
            # print(f"çŸ­æš‚ç­‰å¾…åé‡è¯•: queue_dir ä¸ºç©º\n")
            continue
        # å­¦ä¹ å™¨id-é‡‡æ ·å™¨æœ€æ–°æ¨¡å‹id > æœ€å¤§å»¶è¿Ÿ,  çŸ­æš‚ç­‰å¾…åé‡è¯•
        elif  learner_model_id - queue_dir_max_model_id > max_diff_step:
            time.sleep(1.0)
            # print(f"çŸ­æš‚ç­‰å¾…åé‡è¯•: å­¦ä¹ å™¨id({learner_model_id})-é‡‡æ ·å™¨æœ€æ–°æ¨¡å‹id({queue_dir_max_model_id}) > æœ€å¤§å»¶è¿Ÿ({max_diff_step})\n")
            continue
        # æ™®é€šé‡è¦æ€§é‡‡æ ·
        else:
            sorted_processing_dir = sorted(list(Path(processing_dir).glob("data_*.pt")))
            num_files_of_processing = len(sorted_processing_dir)
            sorted_processing_dir = sorted_processing_dir[-256:]
            processing_max_model_id, _ = get_max_model_id(sorted_processing_dir, rank)

            # æƒ…å†µ-1 queueé‡Œé¢å‡ºç°æ–°çš„idçš„æ•°æ®
            if processing_max_model_id < queue_dir_max_model_id:
                path_of_data_to_learn = path_in_queue_max
                history_advs = None
            # æƒ…å†µ-2 queueé‡Œé¢æ²¡æœ‰å‡ºç°æ–°çš„idçš„æ•°æ®ï¼Œä¼˜å…ˆå»å¯»æ‰¾æ»‘åŠ¨çª—å£å†…çš„æœ€æ—§æ•°æ®ï¼ˆè€Œä¸æ˜¯ä¼˜å…ˆå»å­¦ä¹ æœ€æ–°idä¸‹æ²¡æœ‰å­¦å®Œçš„æ•°æ®ï¼‰
            else:
                theory_min_id = max(processing_max_model_id - max_diff_step,0) # æ­¤å¤„ä¸æ˜¯ä»¥global-stepè®¡ç®—ï¼Œè€Œæ˜¯ä»¥ processing_max_model_id
                # å¯»æ‰¾æ»‘çª—å†…çš„æœ€æ—§æ•°æ®
                queue_dir_min_model_id, path_in_queue_min = get_min_model_id(sorted_queue_dir, theory_min_id, rank)
                # æ²¡æœ‰æœ€æ–°ï¼ˆæŒ‡æ–°çš„idï¼‰æ•°æ®åˆ°è¾¾æ—¶ï¼Œæ°¸è¿œå­¦ä¹ æ»‘çª—å†…æœ€æ—§çš„æ•°æ®
                path_of_data_to_learn = path_in_queue_min
                # æƒ…å†µ2.1 æ²¡æœ‰æ›´æ—§æ•°æ®
                if queue_dir_min_model_id == processing_max_model_id:
                    history_advs= None
                # æƒ…å†µ2.2 æ‰¾åˆ°æ›´æ—§æ•°æ®ï¼Œå¯ä»¥é€€ç«
                else:
                    history_advs = []
                    aix_track = []
                    id2pathlist = {id: [] for id in range(queue_dir_min_model_id + 1, processing_max_model_id + 1)}

                    for data_path in sorted_processing_dir:
                        id =  get_model_id_from_name(data_path.name)
                        # é€€ğŸ”¥æ»‘çª—å†…çš„æ•°æ®
                        if id > queue_dir_min_model_id and id <= processing_max_model_id:
                            id2pathlist[id].append(data_path)
                    ####################################################################################################
                    # sampler_per_token_logps = None
                    for id in range(queue_dir_min_model_id + 1, processing_max_model_id + 1):
                        if len(id2pathlist[id]) == 0:
                            continue
                        # å¯èƒ½å­˜åœ¨å¤šæ¡é€€ç«è·¯å¾„ï¼Œéšæœºé€‰ä¸€æ¡ï¼Œç¡®ä¿å„ä¸ªè¿›ç¨‹é€‰æ‹©åŒä¸€ä¸ªæ—¶é—´æˆ³
                        ais_data_filepath_idx = random.choice(range(len(id2pathlist[id]) // world_size))
                        wait_for_everyone()
                        ais_data_filepath_idx = broadcast(torch.tensor(ais_data_filepath_idx,dtype=torch.int32,device=f"cuda:{rank}"), from_process=0).item()
                        ais_data_filepath = id2pathlist[id][ais_data_filepath_idx * world_size: (1+ais_data_filepath_idx) * world_size][rank]
                        # print(f"ã€æ‰¾åˆ°é€€ç«æ•°æ®ã€‘ rank-{rank}: data_model_id:{data_model_id}  processed_max_model_id:{processed_max_model_id}   self_filename:{filename}")
                        ais_data = torch.load(ais_data_filepath, map_location='cpu', weights_only=False)

                        history_advs.append(ais_data['advantages'].unsqueeze(1))
                        aix_track.append(f"{id}")
                        # è®°å½•é€€ç«è·¯å¾„ä¸Šä¸Šæœ€åä¸€ä¸ªæ ·æœ¬x0çš„logpï¼Œç”¨äºè®¡ç®—é‡è¦æ€§é‡‡æ ·\frac{f0(x0)}{f1(x0)}
                        # if id == processing_max_model_id:
                        #     sampler_per_token_logps = ais_data['sampler_per_token_logps']
                    # assert sampler_per_token_logps is not None
                    history_advs = torch.cat(history_advs, dim=1)
                    if rank == 0:
                        print(f"ã€Œæ‰¾åˆ°é€€ç«æ•°æ®ã€ {'->'.join(aix_track)} (å·²å­¦ä¹ çš„æœ€æ–°æ•°æ®idä¸ºï¼š{processing_max_model_id})")

            data_to_learn = torch.load(path_of_data_to_learn, map_location='cpu', weights_only=False)
            wait_for_everyone()
            os.rename(queue_dir / path_of_data_to_learn.name, processing_dir / path_of_data_to_learn.name)
            data_to_learn['history_advs'] = history_advs if history_advs is not None else torch.zeros([self.args.per_device_train_batch_size * self.args.gradient_accumulation_steps, 1])
            # # print(f"ã€é€€ç«é‡è¦æ€§é‡‡æ ·æ•°æ®ã€‘:{processing_path}")
            data_to_learn['metrics']['train']['num_files_of_queue'] = [num_files_of_queue]
            data_to_learn['metrics']['train']['num_files_of_prcessing'] = [num_files_of_processing]
            data_to_learn['metrics']['train']['queue_dir_max_model_id'] = [queue_dir_max_model_id]
            data_to_learn['metrics']['train']['processed_max_model_id'] = [processing_max_model_id]
            return data_to_learn


# =================================================================================
# 2. æ¨¡å‹åŒæ­¥å›è°ƒ (ä¸ä¹‹å‰ç›¸åŒ)
# =================================================================================
# async_utils.py

class SamplerSyncCallback(TrainerCallback):
    """
    ä¸€ä¸ªå›è°ƒï¼Œç”¨äºåœ¨è®­ç»ƒæ­¥éª¤ç»“æŸæ—¶ï¼Œå®šæœŸå°†å­¦ä¹ å™¨çš„æ¨¡å‹æƒé‡åŒæ­¥ç»™é‡‡æ ·å™¨ã€‚
    """
    def __init__(self, trainer, sync_weights_path: Path, sync_steps: int): # <--- æ–°å¢ trainer å‚æ•°
        self.trainer = trainer  # <--- å°† trainer å­˜ä¸ºæˆå‘˜å˜é‡
        self.sync_weights_path = sync_weights_path
        self.sync_steps = sync_steps
        self.last_synced_step = -1

    def on_step_end(self, args: TrainingArguments, state, control, model: nn.Module, **kwargs):
        """
        åœ¨æ¯ä¸ªæ¢¯åº¦æ›´æ–°æ­¥éª¤çš„æœ«å°¾è¢«è°ƒç”¨ã€‚
        """
        if state.global_step > self.last_synced_step and state.global_step % self.sync_steps == 0:
            self.last_synced_step = state.global_step
            if state.is_world_process_zero:
                unwrapped_model = self.trainer.accelerator.unwrap_model(model)
                temp_path = self.sync_weights_path.with_suffix(".tmp")
                temp_path.parent.mkdir(parents=True, exist_ok=True)
                torch.save((state.global_step, unwrapped_model.state_dict()), temp_path) # d20250717ä¿®æ”¹
                os.rename(temp_path, self.sync_weights_path)
                print(f"[Learner] Step {state.global_step}: Synced weights for sampler at {self.sync_weights_path}")
